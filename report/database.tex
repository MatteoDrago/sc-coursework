\subsection{Database management}

Considering the huge amount of ratings contained in the CSV file, we decided to implement \textit{SQLite} in our architecture in order to build a database where to store our collection of values. Each row of the database's table consists of three entries: the first one contains the user's id, the second one the item's identification while the third represents the rate given to that particular item by the user; in this way we obtained a one-to-one correspondence between database and training dataset. 

In order to speed up access to the database, several indexes were created. An index called \textit{users\_idx} was set up for fast retrieving of data via user ID, while another one called \textit{items\_idx} has been used the same function, this time using an item-based search. Finally we used also the index (\textit{itemsusers\_idx}), used for researches based on a double key build with user and item IDs. When dealing with big databases like this one, the creation of an index is fundamental and in our case speeded up database access of almost $100$ times, comparing with no indexes case.

Moreover, considering that eq. \ref{eq_1} needs the average of the rates of each user, we constructed a dedicated database. Thanks to that, we don't need to recompute thousands of time the same average value. Obviously, also this database contains an index based on the user identification value. 

The similarity matrix will be stored in a database formatted as follow: each row contains within the first two columns a couple of IDs that identify two users, while the third columns contains the pearson coefficient of the pair. An index between column $1$ and $2$ will be created; this index is really important since the database storing similarity values has a disk size of $\approx 60$ GByte. For saving similarity values in the database, we adopted \textit{sqlite} "BEGIN COMMIT" constructor, as reported in method \textit{runTransaction}. Default writing operation takes long time especially considering the high dimension of similarity matrix. In this way, we can insert new values in the matrix every \textit{k} values are found, therefore reducing considerably the total writing time.


The last database that we used is the one containing the predicted rating, formatted according to CSV file \textit{"comp3208-2017-test"}.

